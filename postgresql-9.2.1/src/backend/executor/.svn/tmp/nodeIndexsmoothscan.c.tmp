/*-------------------------------------------------------------------------
 * renata
 * nodeIndexsmoothscan.c
 *	  Routines to support indexed scans of relations
 *
 * IDENTIFICATION
 *	  src/backend/executor/nodeIndexsmoothscan.c
 *
 *-------------------------------------------------------------------------
 */
/*
 * INTERFACE ROUTINES
 *		ExecIndexSmoothScan			scans a relation using an index
 *		IndexSmoothNext				retrieve next tuple using index
 *		ExecInitIndexSmoothScan		creates and initializes state info.
 *		ExecReScanIndexSmoothScan	rescans the indexed relation.
 *		ExecEndIndexSmoothScan		releases all storage.
 *		ExecIndexSmoothMarkPos		marks scan position.
 *		ExecIndexSmoothRestrPos		restores scan position.
 */
#include "postgres.h"

#include "access/nbtree.h"
#include "access/relscan.h"
#include "executor/execdebug.h"
#include "executor/nodeIndexsmoothscan.h"
#include "optimizer/clauses.h"
#include "utils/array.h"
#include "utils/lsyscache.h"
#include "utils/memutils.h"
#include "utils/rel.h"

#include "miscadmin.h"
#include "optimizer/cost.h"




static TupleTableSlot *IndexSmoothNext(IndexSmoothScanState *node);

/* renata
 * decladation of additional methods for index smooth scan
 * (mostly for dealing with result hash table)
 *
 * */
//ResultCache *
//smooth_resultcache_create_empty(long maxbytes);
//
//static void
//smooth_resultcache_create(ResultCache *res_cache);
//
//void
//smooth_resultcache_free(ResultCache *cache);
//
//
//static ResultCacheEntry *
//smooth_resultcache_get_resultentry(ResultCache *cache, BlockNumber pageno);
//
//static  ResultCacheEntry *
//smooth_resultcache_find_resultentry(ResultCache *cache, BlockNumber pageno);

/***************************************************************************************/
ResultCache *
smooth_resultcache_create_empty(long maxbytes);

static void
smooth_resultcache_create(ResultCache *res_cache, uint32 tup_length);

void
smooth_resultcache_free(ResultCache *cache);


static ResultCacheEntry *
smooth_resultcache_get_resultentry(ResultCache *cache, HeapTuple tpl, BlockNumber blknum);

static  ResultCacheEntry *
smooth_resultcache_find_resultentry(ResultCache *cache, TID tid);

static
TID form_tuple_id(HeapTuple tpl, BlockNumber blk);
//
///*Smooth Operators
// * Result Cache methods*/
///* renata
// * This function only prepares for creating hash_table, while the hash table is actually created
// * in the function smooth_create_resultcache */
//
//ResultCache *
//smooth_resultcache_create_empty(long maxbytes)
//{
//	ResultCache  *result;
//	long		nbuckets;
//
//	/* Create ResultCache*/
//	result = makeNode(ResultCache);
//
//	result->mcxt = CurrentMemoryContext;
//	result->status = SS_EMPTY;
//
//	/*
//	 * Estimate number of hashtable entries we can have within maxbytes. This
//	 * estimates the hash overhead at MAXALIGN(sizeof(HASHELEMENT)) plus a
//	 * pointer per hash entry, which is crude but good enough for our purpose.
//	 * Also count an extra Pointer per entry for the arrays created during
//	 * iteration readout.
//	 */
//	/* to do - I should calculate the size of ResultCacheEntry by hand */
//	nbuckets = maxbytes /
//		(MAXALIGN(sizeof(HASHELEMENT)) + MAXALIGN(sizeof(ResultCacheEntry))
//		 + sizeof(Pointer) + sizeof(Pointer));
//	nbuckets = Min(nbuckets, INT_MAX - 1);		/* safety limit */
//	nbuckets = Max(nbuckets, 16);		/* sanity limit */
//	result->maxentries = (int) nbuckets;
//
//	return result;
//}
//
///*
// *
// * Actually create the hashtable.  Since this is a moderately expensive
// * proposition, we don't do it until we have to.
// */
//
//static void
//smooth_resultcache_create(ResultCache *res_cache)
//{
//	HASHCTL		hash_ctl;
//
//	Assert(res_cache != NULL);
//
//	/* Create the hashtable proper */
//	MemSet(&hash_ctl, 0, sizeof(hash_ctl));
//	hash_ctl.keysize = sizeof(BlockNumber);
//	hash_ctl.entrysize = sizeof(ResultCacheEntry);
//	hash_ctl.hash = tag_hash;
//	hash_ctl.hcxt = res_cache->mcxt;
//	res_cache->hashtable = hash_create("ResultCache",
//											 128,	/* start small and extend */
//											 &hash_ctl,
//											 HASH_ELEM | HASH_FUNCTION | HASH_CONTEXT | HASH_SMOOTH);
//
//
//	res_cache->status = SS_HASH;
//}
//
///*
// * tbm_add_tuples - add some tuple IDs to a TIDBitmap
// *
// * If recheck is true, then the recheck flag will be set in the
// * TBMIterateResult when any of these tuples are reported out.
// */
//
//
//bool
//smooth_resultcache_add_tuple(ResultCache *cache, const BlockNumber blk, const OffsetNumber off, const HeapTuple tuple)
//{
//		ResultCacheEntry *page;
//		Size tupleSize;
//
//		/* safety check to ensure we don't overrun bit array bounds */
//		if (off < 1 || off > MaxHeapTuplesPerPage)
//			elog(ERROR, "tuple offset out of range: %u", off);
//
//
//		page = smooth_resultcache_get_resultentry(cache, blk);
//		if(page){
//			/* add tuple */
//			/* 1. get tuple size */
//			tupleSize = tuple->t_len;
//
//			/* 2. mark the offset of this tuple*/
//			page->tupleInfo[page->numTuples].tupleOffset = page->nextTupleOffset;
//			page->tupleInfo[page->numTuples].heapTID = tuple->t_self;
//
//			/* 3. copy tuple in the page cache (this is a bucket for the hash table)*/
//			memcpy(page->tuples + page->nextTupleOffset, tuple->t_data, tupleSize);
//			/* 4. note where next tuple should start */
//			page->nextTupleOffset += MAXALIGN(tupleSize);
//
//			/* 5. increase number of tuples for a page*/
//			page->numTuples++;
//			return true;
//		}else{
//			return false;
//		}
//
//}
//
//
//bool
//smooth_resultcache_find_tuple(ResultCache *cache, HeapTuple tuple)
//{
//	BlockNumber blk;
//	ResultCacheEntry *pageCache;
//	bool found = false;
//	int i;
//
//
//	blk = ItemPointerGetBlockNumber(&(tuple->t_self));
//	pageCache = smooth_resultcache_find_resultentry(cache, blk);
//
//	/* if we have a bucket for this block */
//	if (pageCache){
//		for(i = 0; i < pageCache->numTuples; i++){
//			/* if we have that tuple in the cache */
//			if(pageCache->tupleInfo[i].heapTID.ip_posid == tuple->t_self.ip_posid){
//
//				tuple->t_data = ((HeapTupleHeader) (pageCache->tuples + pageCache->tupleInfo[i].tupleOffset));
//				found = true;
//				break;
//			}
//		}
//	}
//	return found;
//}
///* This method returns ResultCacheEntry if exists, if not NULL is returned */
//
//static  ResultCacheEntry *
//smooth_resultcache_find_resultentry(ResultCache *cache, BlockNumber pageno)
//{
//	ResultCacheEntry *page;
//
//	if (cache->nentries == 0)		/* in case pagetable doesn't exist */
//		return NULL;
//
//	if (cache->status == SS_EMPTY)
//	{
//		return NULL;
//	}
//
//	page = (ResultCacheEntry *) hash_search(cache->hashtable,
//										  (void *) &pageno,
//										  HASH_FIND, NULL);
//
//	return page;
//}
//
// /* This method returns ResultCacheEntry if exists, if not new one is created and returned */
//
///*
// * This may cause the table to exceed the desired memory size.
// */
//static ResultCacheEntry *
//smooth_resultcache_get_resultentry(ResultCache *cache, BlockNumber pageno)
//{
//	ResultCacheEntry *page;
//	bool		found;
//
//	if(cache->status == SS_EMPTY){
//		smooth_resultcache_create(cache);
//	}
//	if(cache->status == SS_HASH){
//		/* Look up or create an entry */
//		page = (ResultCacheEntry *) hash_search(cache->hashtable,
//												  (void *) &pageno,
//												  HASH_ENTER, &found);
//	}else{
//		/* either last or full */
//		/* WE CANNOT CREATE ADD MORE PAGES - so we can only add tuples to existing pages */
//		page = (ResultCacheEntry *) hash_search(cache->hashtable,
//												  (void *) &pageno,
//												  HASH_FIND, &found);
//	}
//	/*checking if hash table is full*/
//	if(page){
//		/* Initialize it if not present before */
//		if (!found)
//		{
//			MemSet(page, 0, sizeof(ResultCacheEntry));
//			page->blockID = pageno;
//			page->numTuples = 0;
//			page->nextTupleOffset = 0;
//			/* must count it too */
//			cache->nentries++;
//			cache->npages++;
//			if (cache->npages == cache->maxentries){
//				printf("\nNO MORE PAGES ARE SUPPOSED TO BE ADDED IN THE CACHE. FULL! \n ");
//				cache->status = SS_FULL;
//			}
//		}
//	}else{
//		printf("\nHash table is full!\n");
//		cache->status = SS_FULL;
//	}
//
//	return page;
//}


/*
 * smooth_free_result_cache - free ResultCache
 */
void
smooth_resultcache_free(ResultCache *cache)
{	printf("\n Number of entries is %d, max is %d", cache->nentries, cache->maxentries);
	if (cache->hashtable)
		hash_destroy(cache->hashtable);

	pfree(cache);
}

TID form_tuple_id(HeapTuple tpl, BlockNumber blknum)
{
	//TID tid =  (blknum << 16) | tpl->t_self.ip_posid;
	TID temp = (TID) blknum;
	TID tid = (TID) ((temp << 32) | ((uint32)tpl->t_self.ip_posid));

	//printf("\n Blok is: %u, Offset: %u, TID: %lu \n", blknum, tpl->t_self.ip_posid, tid);
	return tid;
}

/****************************************************************************************************
/**SOLUTION WHERE ONE KEY = TID, VALUE = TUPLE */
/*Smooth Operators
 * Result Cache methods*/
/* renata
 * This function only prepares for creating hash_table, while the hash table is actually created
 * in the function smooth_create_resultcache */

ResultCache *
smooth_resultcache_create_empty(long maxbytes)
{
	ResultCache  *result;

	/* Create ResultCache*/
	result = makeNode(ResultCache);

	result->mcxt = CurrentMemoryContext;
	result->status = SS_EMPTY;
	result->size = maxbytes;

	return result;
}

/*
 *
 * Actually create the hashtable.  Since this is a moderately expensive
 * proposition, we don't do it until we have to.
 */

static void
smooth_resultcache_create(ResultCache *res_cache, uint32 tup_length)
{
	HASHCTL		hash_ctl;
	long		nbuckets;

	Assert(res_cache != NULL);

	/*
	 * Estimate number of hashtable entries we can have within maxbytes. This
	 * estimates the hash overhead at MAXALIGN(sizeof(HASHELEMENT)) plus a
	 * pointer per hash entry, which is crude but good enough for our purpose.
	 * Also count an extra Pointer per entry for the arrays created during
	 * iteration readout.
	 */
	/* to do - I should calculate the size of ResultCacheEntry by hand */

//	nbuckets = res_cache->size /
//		(MAXALIGN(sizeof(HASHELEMENT)) + MAXALIGN(sizeof(ResultCacheEntry) + (tup_length))
//		 + sizeof(Pointer) + sizeof(Pointer));
	nbuckets = res_cache->size /
			(MAXALIGN(sizeof(HASHELEMENT)) + MAXALIGN(sizeof(TID) + (tup_length))
			 + sizeof(Pointer) + sizeof(Pointer));

	nbuckets = Min(nbuckets, INT_MAX - 1);		/* safety limit */
	nbuckets = Max(nbuckets, 16);		/* sanity limit */

	res_cache->maxentries = (int) nbuckets;
	res_cache->nentries = 0;
	printf("\nmax number of entries in hash table is %ld\n", nbuckets);

	/* Create the hashtable proper */
	MemSet(&hash_ctl, 0, sizeof(hash_ctl));
	hash_ctl.keysize = sizeof(TID);
//	Size entry = sizeof(ResultCacheEntry)+ (tup_length);
	Size entry = MAXALIGN(sizeof(TID)+ (tup_length));
	printf("\nhash table entry size is %d\n", entry);
	//printf("\n Size of result cache entry is %d, tuple length %d \n", sizeof(ResultCacheEntry), tup_length);

	hash_ctl.entrysize = entry;
	hash_ctl.hash = tag_hash;
	hash_ctl.hcxt = res_cache->mcxt;
	res_cache->hashtable = hash_create("ResultCache",
											 128,	/* start small and extend */
											 &hash_ctl,
											 HASH_ELEM | HASH_FUNCTION | HASH_CONTEXT | HASH_SMOOTH);


	res_cache->status = SS_HASH;
}

/*
 * tbm_add_tuples - add some tuple IDs to a TIDBitmap
 *
 * If recheck is true, then the recheck flag will be set in the
 * TBMIterateResult when any of these tuples are reported out.
 */


bool
smooth_resultcache_add_tuple(ResultCache *cache, const BlockNumber blknum, const OffsetNumber off, const HeapTuple tpl)
{
		ResultCacheEntry *resultEntry;

		/* safety check to ensure we don't overrun bit array bounds */
		if (off < 1 || off > MaxHeapTuplesPerPage)
			elog(ERROR, "tuple offset out of range: %u", off);


		resultEntry = smooth_resultcache_get_resultentry(cache, tpl, blknum);
		if(resultEntry){

			//heap_copytuple_into_hash(tpl, &resultEntry->tuple);
			//heap_copytuple_with_tuple(tpl, &resultEntry->tuple);
			//resultEntry->tuple = heap_copytuple(tpl);
			memcpy((char *) &resultEntry->tuple_data, (char *) tpl->t_data, tpl->t_len);
			return true;
		}else{
			return false;
		}

}


bool
smooth_resultcache_find_tuple(ResultCache *cache, HeapTuple tpl, BlockNumber blkn)
{
	ResultCacheEntry *resultCache;
	bool found = false;

	TID tid = form_tuple_id(tpl, blkn);


	resultCache = smooth_resultcache_find_resultentry(cache, tid);

	/* if we have a bucket for this block */
	if (resultCache){
		tpl->t_data = ((HeapTupleHeader) (& resultCache->tuple_data));
		found = true;
	}
	return found;
}


/* This method returns ResultCacheEntry if exists, if not NULL is returned */
static  ResultCacheEntry *
smooth_resultcache_find_resultentry(ResultCache *cache, TID tid)
{
	ResultCacheEntry *resultCache;

	if (cache->nentries == 0)		/* in case pagetable doesn't exist */
		return NULL;

	if (cache->status == SS_EMPTY)
	{
		return NULL;
	}

	resultCache = (ResultCacheEntry *) hash_search(cache->hashtable,
										  (void *) &tid,
										  HASH_FIND, NULL);

	return resultCache;
}

 /* This method returns ResultCacheEntry if exists, if not new one is created and returned */

/*
 * This may cause the table to exceed the desired memory size.
 */
static ResultCacheEntry *
smooth_resultcache_get_resultentry(ResultCache *cache, HeapTuple tpl, BlockNumber blknum)
{
	ResultCacheEntry *resultCache;
	bool		found;
	TID tid = form_tuple_id(tpl, blknum);


	if(cache->status == SS_EMPTY){
		smooth_resultcache_create(cache, tpl->t_len);
	}
	if(cache->status == SS_HASH){
		/* Look up or create an entry */
		resultCache = (ResultCacheEntry *) hash_search(cache->hashtable,
												  (void *) &tid,
												  HASH_ENTER, &found);
	}else{
		/* either last or full */
		/* WE CANNOT CREATE ADD MORE PAGES  */
		resultCache = (ResultCacheEntry *) hash_search(cache->hashtable,
												  (void *) &tid,
												  HASH_FIND, &found);
	}
	/*checking if hash table is full*/
	if(resultCache){
		/* Initialize it if not present before */
		if (!found)
		{
			//MemSet(resultCache, 0, sizeof(ResultCacheEntry) + tpl->t_len);
			MemSet(resultCache, 0, MAXALIGN(sizeof(TID) + tpl->t_len));
			resultCache->tid = tid;

			/* must count it too */
			cache->nentries++;
			//printf("\nCurrent number of entries is %d, max is %d", cache->nentries, cache->maxentries);

			if (cache->nentries == cache->maxentries){
				printf("\nNO MORE PAGES ARE SUPPOSED TO BE ADDED IN THE CACHE. FULL! \n ");
				cache->status = SS_FULL;
			}
		}
	}else{
		printf("\nHash table is full!\n");
		cache->status = SS_FULL;
	}

	return resultCache;
}
/* ----------------------------------------------------------------
 *		IndexNext
 *
 *		Retrieve a tuple from the IndexScan node's currentRelation
 *		using the index specified in the IndexScanState information.
 * ----------------------------------------------------------------
 */
static TupleTableSlot *
IndexSmoothNext(IndexSmoothScanState *node)
{
	EState	   *estate;
	ExprContext *econtext;
	ScanDirection direction;
	IndexScanDesc scandesc;
	HeapTuple	tuple;
	TupleTableSlot *slot;

	/*
	 * extract necessary information from index scan node
	 */
	estate = node->ss.ps.state;
	direction = estate->es_direction;
	/* flip direction if this is an overall backward scan */
	if (ScanDirectionIsBackward(((IndexScan *) node->ss.ps.plan)->indexorderdir))
	{
		if (ScanDirectionIsForward(direction))
			direction = BackwardScanDirection;
		else if (ScanDirectionIsBackward(direction))
			direction = ForwardScanDirection;
	}
	scandesc = node->iss_ScanDesc;
	econtext = node->ss.ps.ps_ExprContext;
	slot = node->ss.ss_ScanTupleSlot;

	/*
	 * ok, now that we have what we need, fetch the next tuple.
	 */
	while ((tuple = indexsmooth_getnext(scandesc, direction, node->iss_NumSmoothScanKeys, node->iss_SmoothScanKeys)) != NULL)
	{
		/*
		 * Store the scanned tuple in the scan tuple slot of the scan state.
		 * Note: we pass 'false' because tuples returned by amgetnext are
		 * pointers onto disk pages and must not be pfree()'d.
		 */
		ExecStoreTuple(tuple,	/* tuple to store */
					   slot,	/* slot to store in */
					   scandesc->xs_cbuf,		/* buffer containing tuple */
					   false);	/* don't pfree */

		/*
		 * If the index was lossy, we have to recheck the index quals using
		 * the fetched tuple.
		 */
		if (scandesc->xs_recheck)
		{
			econtext->ecxt_scantuple = slot;
			ResetExprContext(econtext);
			if (!ExecQual(node->indexqualorig, econtext, false))
			{
				/* Fails recheck, so drop it and loop back for another */
				InstrCountFiltered2(node, 1);
				continue;
			}
		}

		return slot;
	}

	/*
	 * if we get here it means the index scan failed so we are at the end of
	 * the scan..
	 */
	return ExecClearTuple(slot);
}

/*
 * IndexSmoothRecheck -- access method routine to recheck a tuple in EvalPlanQual
 */
static bool
IndexSmoothRecheck(IndexSmoothScanState *node, TupleTableSlot *slot)
{
	ExprContext *econtext;

	/*
	 * extract necessary information from index scan node
	 */
	econtext = node->ss.ps.ps_ExprContext;

	/* Does the tuple meet the indexqual condition? */
	econtext->ecxt_scantuple = slot;

	ResetExprContext(econtext);

	return ExecQual(node->indexqualorig, econtext, false);
}

/* ----------------------------------------------------------------
 *		ExecIndexScan(node)
 * ----------------------------------------------------------------
 */
TupleTableSlot *
ExecIndexSmoothScan(IndexSmoothScanState *node)
{
	/*
	 * If we have runtime keys and they've not already been set up, do it now.
	 */
	if (node->iss_NumRuntimeKeys != 0 && !node->iss_RuntimeKeysReady)
		ExecReScan((PlanState *) node);

	return ExecScan(&node->ss,
					(ExecScanAccessMtd) IndexSmoothNext,
					(ExecScanRecheckMtd) IndexSmoothRecheck);
}

/* ----------------------------------------------------------------
 *		ExecReScanIndexScan(node)
 *
 *		Recalculates the values of any scan keys whose value depends on
 *		information known at runtime, then rescans the indexed relation.
 *
 *		Updating the scan key was formerly done separately in
 *		ExecUpdateIndexScanKeys. Integrating it into ReScan makes
 *		rescans of indices and relations/general streams more uniform.
 * ----------------------------------------------------------------
 */
void
ExecReScanIndexSmoothScan(IndexSmoothScanState *node)
{
	/*
	 * If we are doing runtime key calculations (ie, any of the index key
	 * values weren't simple Consts), compute the new key values.  But first,
	 * reset the context so we don't leak memory as each outer tuple is
	 * scanned.  Note this assumes that we will recalculate *all* runtime keys
	 * on each call.
	 */
	if (node->iss_NumRuntimeKeys != 0)
	{
		ExprContext *econtext = node->iss_RuntimeContext;

		ResetExprContext(econtext);
		ExecIndexEvalRuntimeKeys(econtext,
								 node->iss_RuntimeKeys,
								 node->iss_NumRuntimeKeys);
	}
	node->iss_RuntimeKeysReady = true;

	/* reset index scan */
	index_rescan(node->iss_ScanDesc,
				 node->iss_ScanKeys, node->iss_NumScanKeys,
				 node->iss_OrderByKeys, node->iss_NumOrderByKeys);

	ExecScanReScan(&node->ss);
}







/* ----------------------------------------------------------------
 *		ExecEndIndexScan
 * ----------------------------------------------------------------
 */
void
ExecEndIndexSmoothScan(IndexSmoothScanState *node)
{
	Relation	indexRelationDesc;
	IndexScanDesc indexScanDesc;
	Relation	relation;
	SmoothScanOpaque ss;
	/*
	 * extract information from the node
	 */
	indexRelationDesc = node->iss_RelationDesc;
	indexScanDesc = node->iss_ScanDesc;
	relation = node->ss.ss_currentRelation;

	/*delete smooth info*/
	if (indexScanDesc != NULL){
		ss = (SmoothScanOpaque)node->iss_ScanDesc->smoothInfo;

		printf("\nOverall table size in blocks %ld, prefetcher accumulated %ld \n", ss->rel_nblocks, ss->prefetch_cumul);

		/*delete smooth info*/
		/* we aren't holding any read locks, but gotta drop the pins */
		if (SmoothScanPosIsValid(ss->currPos))
		{
			/* Before leaving current page, deal with any killed items */
			if (ss->numKilled > 0)
				_bt_killitems(indexScanDesc, false);
			ReleaseBuffer(ss->currPos.buf);
			ss->currPos.buf = InvalidBuffer;
		}
		if (SmoothScanPosIsValid(ss->markPos))
		{
			ReleaseBuffer(ss->markPos.buf);
			ss->markPos.buf = InvalidBuffer;
		}
		ss->markItemIndex = -1;

		/* Release storage */
		if (ss->keyData != NULL)
			pfree(ss->keyData);
		/* so->arrayKeyData and so->arrayKeys are in arrayContext */
		if (ss->arrayContext != NULL)
			MemoryContextDelete(ss->arrayContext);
		if (ss->killedItems != NULL)
			pfree(ss->killedItems);
		if (ss->currTuples != NULL)
			pfree(ss->currTuples);
	//	if(ss->bs_tovispages != NULL)
	//		bms_free(ss->bs_tovispages);
		if(ss->bs_vispages != NULL)
			bms_free(ss->bs_vispages);

		if(ss->result_cache != NULL)
		{
			smooth_resultcache_free(ss->result_cache);
		}

		/* so->markTuples should not be pfree'd, see btrescan */
		pfree(ss);
	}

	/*
	 * Free the exprcontext(s) ... now dead code, see ExecFreeExprContext
	 */
#ifdef NOT_USED
	ExecFreeExprContext(&node->ss.ps);
	if (node->iss_RuntimeContext)
		FreeExprContext(node->iss_RuntimeContext, true);
#endif

	/*
	 * clear out tuple table slots
	 */
	ExecClearTuple(node->ss.ps.ps_ResultTupleSlot);
	ExecClearTuple(node->ss.ss_ScanTupleSlot);

	/*
	 * close the index relation (no-op if we didn't open it)
	 */
	if (indexScanDesc)
		index_endscan(indexScanDesc);
	if (indexRelationDesc)
		index_close(indexRelationDesc, NoLock);

	/*
	 * close the heap relation.
	 */
	ExecCloseScanRelation(relation);
}

/* ----------------------------------------------------------------
 *		ExecIndexMarkPos
 * ----------------------------------------------------------------
 */
void
ExecIndexSmoothMarkPos(IndexSmoothScanState *node)
{
	index_markpos(node->iss_ScanDesc);
}

/* ----------------------------------------------------------------
 *		ExecIndexRestrPos
 * ----------------------------------------------------------------
 */
void
ExecIndexSmoothRestrPos(IndexSmoothScanState *node)
{
	index_restrpos(node->iss_ScanDesc);
}




/* ----------------------------------------------------------------
 *		ExecInitIndexScan
 *
 *		Initializes the index scan's state information, creates
 *		scan keys, and opens the base and index relations.
 *
 *		Note: index scans have 2 sets of state information because
 *			  we have to keep track of the base relation and the
 *			  index relation.
 * ----------------------------------------------------------------
 */
IndexSmoothScanState *
ExecInitIndexSmoothScan(IndexSmoothScan *node, EState *estate, int eflags)
{
	IndexSmoothScanState *indexstate;
	Relation	currentRelation;
	bool		relistarget;
	SmoothScanOpaque ss;
	ResultCache *resultCache;

	/*
	 * create state structure
	 */
	indexstate = makeNode(IndexSmoothScanState);
	indexstate->ss.ps.plan = (Plan *) node;
	indexstate->ss.ps.state = estate;

	/*
	 * Miscellaneous initialization
	 *
	 * create expression context for node
	 */
	ExecAssignExprContext(estate, &indexstate->ss.ps);

	indexstate->ss.ps.ps_TupFromTlist = false;

	if(node->orderby){
		printf("\nOrder should be respected\n");
	}else{
		printf("\nOrder does NOT have to be respected\n");
	}

	/*
	 * initialize child expressions
	 *
	 * Note: we don't initialize all of the indexqual expression, only the
	 * sub-parts corresponding to runtime keys (see below).  Likewise for
	 * indexorderby, if any.  But the indexqualorig expression is always
	 * initialized even though it will only be used in some uncommon cases ---
	 * would be nice to improve that.  (Problem is that any SubPlans present
	 * in the expression must be found now...)
	 */
	indexstate->ss.ps.targetlist = (List *)
		ExecInitExpr((Expr *) node->scan.plan.targetlist,
					 (PlanState *) indexstate);
	indexstate->ss.ps.qual = (List *)
		ExecInitExpr((Expr *) node->scan.plan.qual,
					 (PlanState *) indexstate);
	indexstate->indexqualorig = (List *)
		ExecInitExpr((Expr *) node->indexqualorig,
					 (PlanState *) indexstate);

	/*
	 * tuple table initialization
	 */
	ExecInitResultTupleSlot(estate, &indexstate->ss.ps);
	ExecInitScanTupleSlot(estate, &indexstate->ss);

	/*
	 * open the base relation and acquire appropriate lock on it.
	 */
	currentRelation = ExecOpenScanRelation(estate, node->scan.scanrelid);

	indexstate->ss.ss_currentRelation = currentRelation;
	indexstate->ss.ss_currentScanDesc = NULL;	/* no heap scan here */

	/*
	 * get the scan type from the relation descriptor.
	 */
	ExecAssignScanType(&indexstate->ss, RelationGetDescr(currentRelation));

	/*
	 * Initialize result tuple type and projection info.
	 */
	ExecAssignResultTypeFromTL(&indexstate->ss.ps);
	ExecAssignScanProjectionInfo(&indexstate->ss);

	/*
	 * If we are just doing EXPLAIN (ie, aren't going to run the plan), stop
	 * here.  This allows an index-advisor plugin to EXPLAIN a plan containing
	 * references to nonexistent indexes.
	 */
	if (eflags & EXEC_FLAG_EXPLAIN_ONLY)
		return indexstate;

	/*
	 * Open the index relation.
	 *
	 * If the parent table is one of the target relations of the query, then
	 * InitPlan already opened and write-locked the index, so we can avoid
	 * taking another lock here.  Otherwise we need a normal reader's lock.
	 */
	relistarget = ExecRelationIsTargetRelation(estate, node->scan.scanrelid);
	indexstate->iss_RelationDesc = index_open(node->indexid,
									 relistarget ? NoLock : AccessShareLock);

	/*
	 * Initialize index-specific scan state
	 */
	indexstate->iss_RuntimeKeysReady = false;
	indexstate->iss_RuntimeKeys = NULL;
	indexstate->iss_NumRuntimeKeys = 0;

	/*
	 * build the index scan keys from the index qualification
	 */
	ExecIndexBuildScanKeys((PlanState *) indexstate,
						   indexstate->iss_RelationDesc,
						   node->indexqual,
						   false,
						   &indexstate->iss_ScanKeys,
						   &indexstate->iss_NumScanKeys,
						   &indexstate->iss_RuntimeKeys,
						   &indexstate->iss_NumRuntimeKeys,
						   NULL,	/* no ArrayKeys */
						   NULL);

	/*
	 * build the index scan keys from the index qualification
	 */
	ExecIndexBuildSmoothScanKeys((PlanState *) indexstate,
						   indexstate->iss_RelationDesc,
						   node->indexqualorig,
						   false,
						   &indexstate->iss_SmoothScanKeys,
						   &indexstate->iss_NumSmoothScanKeys,
						   &indexstate->iss_RuntimeKeys,
						   &indexstate->iss_NumRuntimeKeys,
						   NULL,	/* no ArrayKeys */
						   NULL);


	/*
	 * any ORDER BY exprs have to be turned into scankeys in the same way
	 */
	ExecIndexBuildScanKeys((PlanState *) indexstate,
						   indexstate->iss_RelationDesc,
						   node->indexorderby,
						   true,
						   &indexstate->iss_OrderByKeys,
						   &indexstate->iss_NumOrderByKeys,
						   &indexstate->iss_RuntimeKeys,
						   &indexstate->iss_NumRuntimeKeys,
						   NULL,	/* no ArrayKeys */
						   NULL);

	/*
	 * If we have runtime keys, we need an ExprContext to evaluate them. The
	 * node's standard context won't do because we want to reset that context
	 * for every tuple.  So, build another context just like the other one...
	 * -tgl 7/11/00
	 */
	if (indexstate->iss_NumRuntimeKeys != 0)
	{
		ExprContext *stdecontext = indexstate->ss.ps.ps_ExprContext;

		ExecAssignExprContext(estate, &indexstate->ss.ps);
		indexstate->iss_RuntimeContext = indexstate->ss.ps.ps_ExprContext;
		indexstate->ss.ps.ps_ExprContext = stdecontext;
	}
	else
	{
		indexstate->iss_RuntimeContext = NULL;
	}

	/*
	 * Initialize scan descriptor.
	 */
	indexstate->iss_ScanDesc = index_beginscan(currentRelation,
											   indexstate->iss_RelationDesc,
											   estate->es_snapshot,
											   indexstate->iss_NumScanKeys,
											 indexstate->iss_NumOrderByKeys);

	/*
	 * If no run-time keys to calculate, go ahead and pass the scankeys to the
	 * index AM.
	 */
	if (indexstate->iss_NumRuntimeKeys == 0)
		index_rescan(indexstate->iss_ScanDesc,
					 indexstate->iss_ScanKeys, indexstate->iss_NumScanKeys,
				indexstate->iss_OrderByKeys, indexstate->iss_NumOrderByKeys);

	/* renata */
	/* in initialize Smooth Scan Info */



	ss = (SmoothScanOpaque) palloc(sizeof(SmoothScanOpaqueData));
	ss->currPos.buf = ss->markPos.buf = InvalidBuffer;
	if (indexstate->iss_ScanDesc->numberOfKeys > 0)
		ss->keyData = (ScanKey) palloc(indexstate->iss_ScanDesc->numberOfKeys * sizeof(ScanKeyData));
	else
		ss->keyData = NULL;

	ss->arrayKeyData = NULL;	/* assume no array keys for now */
	ss->numArrayKeys = 0;
	ss->arrayKeys = NULL;
	ss->arrayContext = NULL;

	ss->killedItems = NULL;		/* until needed */
	ss->numKilled = 0;
	/*
	 * We don't know yet whether the scan will be index-only, so we do not
	 * allocate the tuple workspace arrays until btrescan.	However, we set up
	 * scan->xs_itupdesc whether we'll need it or not, since that's so cheap.
	 */
	ss->currTuples = (char *) palloc(BLCKSZ * 2);
	ss->markTuples = ss->currTuples + BLCKSZ;

	ss->currPos.nextTupleOffset = 0;
	ss->markPos.nextTupleOffset = 0;
//	ss->bs_tovispages = NULL;
	ss->bs_vispages = NULL;

	ss->more_data_for_smooth = false;

	ss->nextPageId = InvalidBlockNumber;


	ss->currPos.firstHeapItem   = 0;
	ss->currPos.lastHeapItem    = 0;
	ss->currPos.itemHeapIndex   = 0;
	ss->currPos.nextTupleOffset = 0;

	ss->prefetch_pages = 0;
	ss->prefetch_target = 0;
	ss->prefetch_cumul = 0;

	ss->rel_nblocks = RelationGetNumberOfBlocks(currentRelation);
	/* respect order constraint - yes or no*/
	ss->orderby = node->orderby;

	if (ss->orderby){
		ss->result_cache = smooth_resultcache_create_empty(work_mem * 1024L);
	}else{
		ss->result_cache = NULL;
	}


	/* this should go in initialize smooth info */
	indexstate->iss_ScanDesc->smoothInfo = ss;

	/*
	 * all done.
	 */
	return indexstate;
}

void
ExecIndexBuildSmoothScanKeys(PlanState *planstate, Relation index,
					   List *quals, bool isorderby,
					   ScanKey *scanKeys, int *numScanKeys,
					   IndexRuntimeKeyInfo **runtimeKeys, int *numRuntimeKeys,
					   IndexArrayKeyInfo **arrayKeys, int *numArrayKeys)
{
	ListCell   *qual_cell;
	ScanKey		scan_keys;
	IndexRuntimeKeyInfo *runtime_keys;
	IndexArrayKeyInfo *array_keys;
	int			n_scan_keys;
	int			n_runtime_keys;
	int			max_runtime_keys;
	int			n_array_keys;
	int			j;

	/* Allocate array for ScanKey structs: one per qual */
	n_scan_keys = list_length(quals);
	scan_keys = (ScanKey) palloc(n_scan_keys * sizeof(ScanKeyData));

	/*
	 * runtime_keys array is dynamically resized as needed.  We handle it this
	 * way so that the same runtime keys array can be shared between
	 * indexquals and indexorderbys, which will be processed in separate calls
	 * of this function.  Caller must be sure to pass in NULL/0 for first
	 * call.
	 */
	runtime_keys = *runtimeKeys;
	n_runtime_keys = max_runtime_keys = *numRuntimeKeys;

	/* Allocate array_keys as large as it could possibly need to be */
	array_keys = (IndexArrayKeyInfo *)
		palloc0(n_scan_keys * sizeof(IndexArrayKeyInfo));
	n_array_keys = 0;

	/*
	 * for each opclause in the given qual, convert the opclause into a single
	 * scan key
	 */
	j = 0;
	foreach(qual_cell, quals)
	{
		Expr	   *clause = (Expr *) lfirst(qual_cell);
		ScanKey		this_scan_key = &scan_keys[j++];
		Oid			opno;		/* operator's OID */
		RegProcedure opfuncid;	/* operator proc id used in scan */
		Oid			opfamily;	/* opfamily of index column */
		int			op_strategy;	/* operator's strategy number */
		Oid			op_lefttype;	/* operator's declared input types */
		Oid			op_righttype;
		Expr	   *leftop;		/* expr on lhs of operator */
		Expr	   *rightop;	/* expr on rhs ... */
		AttrNumber	varattno;	/* att number used in scan */

		if (IsA(clause, OpExpr))
		{
			/* indexkey op const or indexkey op expression */
			int			flags = 0;
			Datum		scanvalue;

			opno = ((OpExpr *) clause)->opno;
			opfuncid = ((OpExpr *) clause)->opfuncid;

			/*
			 * leftop should be the index key Var, possibly relabeled
			 */
			leftop = (Expr *) get_leftop(clause);

			if (leftop && IsA(leftop, RelabelType))
				leftop = ((RelabelType *) leftop)->arg;

			Assert(leftop != NULL);

			if (!(IsA(leftop, Var)))
				elog(ERROR, "indexqual doesn't have key on left side");

			varattno = ((Var *) leftop)->varattno;
			if (varattno < 1 )
				elog(ERROR, "bogus index qualification");

//			/*
//			 * We have to look up the operator's strategy number.  This
//			 * provides a cross-check that the operator does match the index.
//			 */
//			opfamily = index->rd_opfamily[varattno - 1];
//
//			get_op_opfamily_properties(opno, opfamily, isorderby,
//									   &op_strategy,
//									   &op_lefttype,
//									   &op_righttype);

			if (isorderby)
				flags |= SK_ORDER_BY;

			/*
			 * rightop is the constant or variable comparison value
			 */
			rightop = (Expr *) get_rightop(clause);

			if (rightop && IsA(rightop, RelabelType))
				rightop = ((RelabelType *) rightop)->arg;

			Assert(rightop != NULL);

			if (IsA(rightop, Const))
			{
				/* OK, simple constant comparison value */
				scanvalue = ((Const *) rightop)->constvalue;
				if (((Const *) rightop)->constisnull)
					flags |= SK_ISNULL;
			}
			else
			{
				/* Need to treat this one as a runtime key */
				if (n_runtime_keys >= max_runtime_keys)
				{
					if (max_runtime_keys == 0)
					{
						max_runtime_keys = 8;
						runtime_keys = (IndexRuntimeKeyInfo *)
							palloc(max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
					}
					else
					{
						max_runtime_keys *= 2;
						runtime_keys = (IndexRuntimeKeyInfo *)
							repalloc(runtime_keys, max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
					}
				}
				runtime_keys[n_runtime_keys].scan_key = this_scan_key;
				runtime_keys[n_runtime_keys].key_expr =
					ExecInitExpr(rightop, planstate);
				runtime_keys[n_runtime_keys].key_toastable =
					TypeIsToastable(op_righttype);
				n_runtime_keys++;
				scanvalue = (Datum) 0;
			}

			/*
			 * initialize the scan key's fields appropriately
			 */
			ScanKeyEntryInitialize(this_scan_key,
								   flags,
								   varattno,	/* attribute number to scan */
								   op_strategy, /* op's strategy */
								   op_righttype,		/* strategy subtype */
								   ((OpExpr *) clause)->inputcollid,	/* collation */
								   opfuncid,	/* reg proc to use */
								   scanvalue);	/* constant */
		}
		else if (IsA(clause, RowCompareExpr))
		{
			/* (indexkey, indexkey, ...) op (expression, expression, ...) */
			RowCompareExpr *rc = (RowCompareExpr *) clause;
			ListCell   *largs_cell = list_head(rc->largs);
			ListCell   *rargs_cell = list_head(rc->rargs);
			ListCell   *opnos_cell = list_head(rc->opnos);
			ListCell   *collids_cell = list_head(rc->inputcollids);
			ScanKey		first_sub_key;
			int			n_sub_key;

			Assert(!isorderby);

			first_sub_key = (ScanKey)
				palloc(list_length(rc->opnos) * sizeof(ScanKeyData));
			n_sub_key = 0;

			/* Scan RowCompare columns and generate subsidiary ScanKey items */
			while (opnos_cell != NULL)
			{
				ScanKey		this_sub_key = &first_sub_key[n_sub_key];
				int			flags = SK_ROW_MEMBER;
				Datum		scanvalue;
				Oid			inputcollation;

				/*
				 * leftop should be the index key Var, possibly relabeled
				 */
				leftop = (Expr *) lfirst(largs_cell);
				largs_cell = lnext(largs_cell);

				if (leftop && IsA(leftop, RelabelType))
					leftop = ((RelabelType *) leftop)->arg;

				Assert(leftop != NULL);

				if (!(IsA(leftop, Var)))
					elog(ERROR, "indexqual doesn't have key on left side");

				varattno = ((Var *) leftop)->varattno;

				/*
				 * We have to look up the operator's associated btree support
				 * function
				 */
				opno = lfirst_oid(opnos_cell);
				opnos_cell = lnext(opnos_cell);

				if (index->rd_rel->relam != BTREE_AM_OID ||
					varattno < 1 )
					elog(ERROR, "bogus RowCompare index qualification");

//				opfamily = index->rd_opfamily[varattno - 1];
//
//				get_op_opfamily_properties(opno, opfamily, isorderby,
//										   &op_strategy,
//										   &op_lefttype,
//										   &op_righttype);
//
//				if (op_strategy != rc->rctype)
//					elog(ERROR, "RowCompare index qualification contains wrong operator");
//
//				opfuncid = get_opfamily_proc(opfamily,
//											 op_lefttype,
//											 op_righttype,
//											 BTORDER_PROC);

				inputcollation = lfirst_oid(collids_cell);
				collids_cell = lnext(collids_cell);

				/*
				 * rightop is the constant or variable comparison value
				 */
				rightop = (Expr *) lfirst(rargs_cell);
				rargs_cell = lnext(rargs_cell);

				if (rightop && IsA(rightop, RelabelType))
					rightop = ((RelabelType *) rightop)->arg;

				Assert(rightop != NULL);

				if (IsA(rightop, Const))
				{
					/* OK, simple constant comparison value */
					scanvalue = ((Const *) rightop)->constvalue;
					if (((Const *) rightop)->constisnull)
						flags |= SK_ISNULL;
				}
				else
				{
					/* Need to treat this one as a runtime key */
					if (n_runtime_keys >= max_runtime_keys)
					{
						if (max_runtime_keys == 0)
						{
							max_runtime_keys = 8;
							runtime_keys = (IndexRuntimeKeyInfo *)
								palloc(max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
						}
						else
						{
							max_runtime_keys *= 2;
							runtime_keys = (IndexRuntimeKeyInfo *)
								repalloc(runtime_keys, max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
						}
					}
					runtime_keys[n_runtime_keys].scan_key = this_sub_key;
					runtime_keys[n_runtime_keys].key_expr =
						ExecInitExpr(rightop, planstate);
					runtime_keys[n_runtime_keys].key_toastable =
						TypeIsToastable(op_righttype);
					n_runtime_keys++;
					scanvalue = (Datum) 0;
				}

				/*
				 * initialize the subsidiary scan key's fields appropriately
				 */
				ScanKeyEntryInitialize(this_sub_key,
									   flags,
									   varattno,		/* attribute number */
									   op_strategy,		/* op's strategy */
									   op_righttype,	/* strategy subtype */
									   inputcollation,	/* collation */
									   opfuncid,		/* reg proc to use */
									   scanvalue);		/* constant */
				n_sub_key++;
			}

			/* Mark the last subsidiary scankey correctly */
			first_sub_key[n_sub_key - 1].sk_flags |= SK_ROW_END;

			/*
			 * We don't use ScanKeyEntryInitialize for the header because it
			 * isn't going to contain a valid sk_func pointer.
			 */
			MemSet(this_scan_key, 0, sizeof(ScanKeyData));
			this_scan_key->sk_flags = SK_ROW_HEADER;
			this_scan_key->sk_attno = first_sub_key->sk_attno;
			this_scan_key->sk_strategy = rc->rctype;
			/* sk_subtype, sk_collation, sk_func not used in a header */
			this_scan_key->sk_argument = PointerGetDatum(first_sub_key);
		}
		else if (IsA(clause, ScalarArrayOpExpr))
		{
			/* indexkey op ANY (array-expression) */
			ScalarArrayOpExpr *saop = (ScalarArrayOpExpr *) clause;
			int			flags = 0;
			Datum		scanvalue;

			Assert(!isorderby);

			Assert(saop->useOr);
			opno = saop->opno;
			opfuncid = saop->opfuncid;

			/*
			 * leftop should be the index key Var, possibly relabeled
			 */
			leftop = (Expr *) linitial(saop->args);

			if (leftop && IsA(leftop, RelabelType))
				leftop = ((RelabelType *) leftop)->arg;

			Assert(leftop != NULL);

			if (!(IsA(leftop, Var) ))
				elog(ERROR, "indexqual doesn't have key on left side");

			varattno = ((Var *) leftop)->varattno;
			if (varattno < 1 )
				elog(ERROR, "bogus index qualification");

//			/*
//			 * We have to look up the operator's strategy number.  This
//			 * provides a cross-check that the operator does match the index.
//			 */
//			opfamily = index->rd_opfamily[varattno - 1];
//
//			get_op_opfamily_properties(opno, opfamily, isorderby,
//									   &op_strategy,
//									   &op_lefttype,
//									   &op_righttype);

			/*
			 * rightop is the constant or variable array value
			 */
			rightop = (Expr *) lsecond(saop->args);

			if (rightop && IsA(rightop, RelabelType))
				rightop = ((RelabelType *) rightop)->arg;

			Assert(rightop != NULL);

			if (index->rd_am->amsearcharray)
			{
				/* Index AM will handle this like a simple operator */
				flags |= SK_SEARCHARRAY;
				if (IsA(rightop, Const))
				{
					/* OK, simple constant comparison value */
					scanvalue = ((Const *) rightop)->constvalue;
					if (((Const *) rightop)->constisnull)
						flags |= SK_ISNULL;
				}
				else
				{
					/* Need to treat this one as a runtime key */
					if (n_runtime_keys >= max_runtime_keys)
					{
						if (max_runtime_keys == 0)
						{
							max_runtime_keys = 8;
							runtime_keys = (IndexRuntimeKeyInfo *)
								palloc(max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
						}
						else
						{
							max_runtime_keys *= 2;
							runtime_keys = (IndexRuntimeKeyInfo *)
								repalloc(runtime_keys, max_runtime_keys * sizeof(IndexRuntimeKeyInfo));
						}
					}
					runtime_keys[n_runtime_keys].scan_key = this_scan_key;
					runtime_keys[n_runtime_keys].key_expr =
						ExecInitExpr(rightop, planstate);

					/*
					 * Careful here: the runtime expression is not of
					 * op_righttype, but rather is an array of same; so
					 * TypeIsToastable() isn't helpful.  However, we can
					 * assume that all array types are toastable.
					 */
					runtime_keys[n_runtime_keys].key_toastable = true;
					n_runtime_keys++;
					scanvalue = (Datum) 0;
				}
			}
			else
			{
				/* Executor has to expand the array value */
				array_keys[n_array_keys].scan_key = this_scan_key;
				array_keys[n_array_keys].array_expr =
					ExecInitExpr(rightop, planstate);
				/* the remaining fields were zeroed by palloc0 */
				n_array_keys++;
				scanvalue = (Datum) 0;
			}

			/*
			 * initialize the scan key's fields appropriately
			 */
			ScanKeyEntryInitialize(this_scan_key,
								   flags,
								   varattno,	/* attribute number to scan */
								   op_strategy, /* op's strategy */
								   op_righttype,		/* strategy subtype */
								   saop->inputcollid,	/* collation */
								   opfuncid,	/* reg proc to use */
								   scanvalue);	/* constant */
		}
		else if (IsA(clause, NullTest))
		{
			/* indexkey IS NULL or indexkey IS NOT NULL */
			NullTest   *ntest = (NullTest *) clause;
			int			flags;

			Assert(!isorderby);

			/*
			 * argument should be the index key Var, possibly relabeled
			 */
			leftop = ntest->arg;

			if (leftop && IsA(leftop, RelabelType))
				leftop = ((RelabelType *) leftop)->arg;

			Assert(leftop != NULL);

			if (!(IsA(leftop, Var) &&
				  ((Var *) leftop)->varno == INDEX_VAR))
				elog(ERROR, "NullTest indexqual has wrong key");

			varattno = ((Var *) leftop)->varattno;

			/*
			 * initialize the scan key's fields appropriately
			 */
			switch (ntest->nulltesttype)
			{
				case IS_NULL:
					flags = SK_ISNULL | SK_SEARCHNULL;
					break;
				case IS_NOT_NULL:
					flags = SK_ISNULL | SK_SEARCHNOTNULL;
					break;
				default:
					elog(ERROR, "unrecognized nulltesttype: %d",
						 (int) ntest->nulltesttype);
					flags = 0;	/* keep compiler quiet */
					break;
			}

			ScanKeyEntryInitialize(this_scan_key,
								   flags,
								   varattno,	/* attribute number to scan */
								   InvalidStrategy,		/* no strategy */
								   InvalidOid,	/* no strategy subtype */
								   InvalidOid,	/* no collation */
								   InvalidOid,	/* no reg proc for this */
								   (Datum) 0);	/* constant */
		}
		else
			elog(ERROR, "unsupported indexqual type: %d",
				 (int) nodeTag(clause));
	}

	Assert(n_runtime_keys <= max_runtime_keys);

	/* Get rid of any unused arrays */
	if (n_array_keys == 0)
	{
		pfree(array_keys);
		array_keys = NULL;
	}

	/*
	 * Return info to our caller.
	 */
	*scanKeys = scan_keys;
	*numScanKeys = n_scan_keys;
	*runtimeKeys = runtime_keys;
	*numRuntimeKeys = n_runtime_keys;
	if (arrayKeys)
	{
		*arrayKeys = array_keys;
		*numArrayKeys = n_array_keys;
	}
	else if (n_array_keys != 0)
		elog(ERROR, "ScalarArrayOpExpr index qual found where not allowed");
}



